'''
generated by Claude
'''
import os
import json
import torch
import librosa
import pandas as pd
from pathlib import Path
from xml.etree import ElementTree as ET
from datasets import Dataset, Audio
from transformers import (
    WhisperFeatureExtractor,
    WhisperTokenizer,
    WhisperProcessor,
    WhisperForConditionalGeneration,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
    DataCollatorForSeq2Seq
)
from dataclasses import dataclass
from typing import Any, Dict, List, Union
import numpy as np

class ELANParser:
    """Parse ELAN files to extract transcription data"""
    
    def __init__(self, elan_file_path):
        self.elan_file = elan_file_path
        self.tree = ET.parse(elan_file_path)
        self.root = self.tree.getroot()
    
    def extract_annotations(self):
        """Extract time-aligned annotations from ELAN file"""
        annotations = []
        
        # Get time slots
        time_slots = {}
        for time_slot in self.root.findall('.//TIME_SLOT'):
            slot_id = time_slot.get('TIME_SLOT_ID')
            time_value = int(time_slot.get('TIME_VALUE'))
            time_slots[slot_id] = time_value / 1000.0  # Convert to seconds
        
        # Extract annotations with transcriptions
        for tier in self.root.findall('.//TIER'):
            tier_id = tier.get('TIER_ID')
            
            # Focus on transcription tiers (adjust tier names as needed)
            if 'transcription' in tier_id.lower() or 'enenlhet' in tier_id.lower():
                for annotation in tier.findall('.//ANNOTATION/ALIGNABLE_ANNOTATION'):
                    start_id = annotation.get('TIME_SLOT_REF1')
                    end_id = annotation.get('TIME_SLOT_REF2')
                    
                    if start_id in time_slots and end_id in time_slots:
                        start_time = time_slots[start_id]
                        end_time = time_slots[end_id]
                        
                        # Get transcription text
                        annotation_value = annotation.find('ANNOTATION_VALUE')
                        if annotation_value is not None and annotation_value.text:
                            text = annotation_value.text.strip()
                            if text:  # Only include non-empty transcriptions
                                annotations.append({
                                    'start_time': start_time,
                                    'end_time': end_time,
                                    'duration': end_time - start_time,
                                    'text': text,
                                    'tier_id': tier_id
                                })
        
        return annotations

def prepare_dataset(elan_files, audio_files, output_dir="processed_data"):
    """Prepare dataset from ELAN and audio files"""
    
    os.makedirs(output_dir, exist_ok=True)
    all_segments = []
    
    for elan_file, audio_file in zip(elan_files, audio_files):
        print(f"Processing {elan_file}")
        
        # Parse ELAN file
        parser = ELANParser(elan_file)
        annotations = parser.extract_annotations()
        
        # Load audio
        audio, sr = librosa.load(audio_file, sr=16000)  # Whisper expects 16kHz
        
        # Extract audio segments
        for i, ann in enumerate(annotations):
            start_sample = int(ann['start_time'] * sr)
            end_sample = int(ann['end_time'] * sr)
            
            # Skip very short segments (< 0.5 seconds)
            if ann['duration'] < 0.5:
                continue
                
            # Skip very long segments (> 30 seconds) - Whisper's limit
            if ann['duration'] > 30:
                continue
            
            segment_audio = audio[start_sample:end_sample]
            
            # Save audio segment
            segment_filename = f"segment_{Path(elan_file).stem}_{i:04d}.wav"
            segment_path = os.path.join(output_dir, segment_filename)
            librosa.output.write_wav(segment_path, segment_audio, sr)
            
            all_segments.append({
                'audio_path': segment_path,
                'transcription': ann['text'],
                'duration': ann['duration'],
                'source_file': Path(elan_file).stem
            })
    
    # Create dataset CSV
    df = pd.DataFrame(all_segments)
    df.to_csv(os.path.join(output_dir, 'dataset.csv'), index=False)
    
    print(f"Created {len(all_segments)} audio segments")
    print(f"Total duration: {df['duration'].sum():.2f} seconds")
    
    return all_segments

def create_whisper_dataset(segments_data, model_name="openai/whisper-small"):
    """Create HuggingFace dataset for Whisper training"""
    
    # Initialize feature extractor and tokenizer
    feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name)
    tokenizer = WhisperTokenizer.from_pretrained(model_name, language="enenlhet", task="transcribe")
    
    # Prepare data for dataset
    audio_paths = [seg['audio_path'] for seg in segments_data]
    transcriptions = [seg['transcription'] for seg in segments_data]
    
    # Create dataset
    dataset = Dataset.from_dict({
        "audio": audio_paths,
        "transcription": transcriptions
    })
    
    # Cast audio column to Audio feature
    dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))
    
    return dataset, feature_extractor, tokenizer

@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    """Data collator for speech-to-text tasks"""
    
    processor: Any
    decoder_start_token_id: int

    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        # Split inputs and labels
        input_features = [{"input_features": feature["input_features"]} for feature in features]
        label_features = [{"input_ids": feature["labels"]} for feature in features]

        batch = self.processor.feature_extractor.pad(input_features, return_tensors="pt")

        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")

        # Replace padding with -100 to ignore loss correctly
        labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)

        # If bos token is appended in previous tokenization step,
        # cut bos token here as it's append later anyways
        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():
            labels = labels[:, 1:]

        batch["labels"] = labels

        return batch

def prepare_dataset_for_training(dataset, processor):
    """Prepare dataset with feature extraction and tokenization"""
    
    def prepare_dataset_fn(batch):
        # Load and process audio
        audio = batch["audio"]
        
        # Compute input features
        input_features = processor.feature_extractor(
            audio["array"], 
            sampling_rate=audio["sampling_rate"]
        ).input_features[0]
        
        # Encode target text
        labels = processor.tokenizer(batch["transcription"]).input_ids
        
        return {
            "input_features": input_features,
            "labels": labels
        }
    
    dataset = dataset.map(
        prepare_dataset_fn,
        remove_columns=dataset.column_names,
        num_proc=4
    )
    
    return dataset

def fine_tune_whisper(
    train_dataset,
    eval_dataset,
    model_name="openai/whisper-small",
    output_dir="whisper-enenlhet",
    num_epochs=10,
    batch_size=8,
    learning_rate=1e-5
):
    """Fine-tune Whisper model"""
    
    # Load model and processor
    model = WhisperForConditionalGeneration.from_pretrained(model_name)
    processor = WhisperProcessor.from_pretrained(model_name, language="enenlhet", task="transcribe")
    
    # Prepare datasets
    train_dataset = prepare_dataset_for_training(train_dataset, processor)
    eval_dataset = prepare_dataset_for_training(eval_dataset, processor)
    
    # Data collator
    data_collator = DataCollatorSpeechSeq2SeqWithPadding(
        processor=processor,
        decoder_start_token_id=model.generation_config.decoder_start_token_id,
    )
    
    # Training arguments
    training_args = Seq2SeqTrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        gradient_accumulation_steps=2,
        learning_rate=learning_rate,
        warmup_steps=500,
        max_steps=1000,  # Adjust based on your data size
        gradient_checkpointing=True,
        fp16=True,
        evaluation_strategy="steps",
        eval_steps=100,
        save_steps=100,
        logging_steps=25,
        report_to=["tensorboard"],
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        push_to_hub=False,
    )
    
    # Initialize trainer
    trainer = Seq2SeqTrainer(
        args=training_args,
        model=model,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=data_collator,
        tokenizer=processor.feature_extractor,
    )
    
    # Start training
    trainer.train()
    
    # Save the model
    trainer.save_model()
    processor.save_pretrained(output_dir)
    
    return trainer

def main():
    """Main training pipeline"""
    
    # Define your file paths
    elan_files = [
        "path/to/your/file1.eaf",
        "path/to/your/file2.eaf",
        # Add all your ELAN files
    ]
    
    audio_files = [
        "path/to/your/audio1.wav",
        "path/to/your/audio2.wav", 
        # Add corresponding audio files
    ]
    
    print("Step 1: Preparing dataset from ELAN files...")
    segments = prepare_dataset(elan_files, audio_files)
    
    print("Step 2: Creating HuggingFace dataset...")
    dataset, feature_extractor, tokenizer = create_whisper_dataset(segments)
    
    print("Step 3: Splitting dataset...")
    # Split into train/eval (80/20)
    train_test_split = dataset.train_test_split(test_size=0.2, seed=42)
    train_dataset = train_test_split['train']
    eval_dataset = train_test_split['test']
    
    print(f"Training samples: {len(train_dataset)}")
    print(f"Evaluation samples: {len(eval_dataset)}")
    
    print("Step 4: Fine-tuning Whisper...")
    trainer = fine_tune_whisper(
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        model_name="openai/whisper-small",  # Start with small model
        output_dir="whisper-enenlhet-finetuned",
        num_epochs=15,
        batch_size=4,  # Adjust based on your GPU memory
        learning_rate=1e-5
    )
    
    print("Fine-tuning completed!")

def test_model(model_path, audio_file):
    """Test the fine-tuned model on new audio"""
    
    # Load fine-tuned model
    model = WhisperForConditionalGeneration.from_pretrained(model_path)
    processor = WhisperProcessor.from_pretrained(model_path)
    
    # Load and process audio
    audio, sr = librosa.load(audio_file, sr=16000)
    input_features = processor(audio, sampling_rate=16000, return_tensors="pt").input_features
    
    # Generate transcription
    with torch.no_grad():
        predicted_ids = model.generate(input_features)
        transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
    
    return transcription

if __name__ == "__main__":
    main()