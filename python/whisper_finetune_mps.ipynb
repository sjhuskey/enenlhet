{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "208c9957",
   "metadata": {},
   "source": [
    "# Whisper Fine-tuning for Enenlhet Language (MPS Accelerated)\n",
    "\n",
    "This notebook fine-tunes OpenAI's Whisper model on Enenlhet audio transcription data using Apple Silicon MPS acceleration with robust tensor handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "044f87c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Audio\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import random\n",
    "from transformers import (\n",
    "    WhisperFeatureExtractor, \n",
    "    WhisperTokenizer, \n",
    "    WhisperProcessor, \n",
    "    WhisperForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "from codecarbon import EmissionsTracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df8a1158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using MPS device for accelerated training\n",
      "PyTorch version: 2.5.1\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Device setup - Use MPS with robust tensor handling\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "# Check for MPS availability and use it with our robust tensor handling\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(f\"‚úÖ Using MPS device for accelerated training\")\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"‚ö†Ô∏è MPS not available, falling back to CPU\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if device.type == \"mps\":\n",
    "    torch.mps.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "195ba3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data path: ../data/have_transcripts/dataset.jsonl\n",
      "Output directory: ../whisper-finetuned\n",
      "Log directory: ../logs\n"
     ]
    }
   ],
   "source": [
    "# File paths\n",
    "data_path = \"../data/have_transcripts/dataset.jsonl\"\n",
    "output_dir = \"../whisper-finetuned\"\n",
    "log_dir = \"../logs\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Data path: {data_path}\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(f\"Log directory: {log_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28bf83ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading openai/whisper-small...\n",
      "Model loaded and moved to mps\n",
      "Model loaded and moved to mps\n"
     ]
    }
   ],
   "source": [
    "# Load Whisper model and processor\n",
    "model_name = \"openai/whisper-small\"\n",
    "\n",
    "print(f\"Loading {model_name}...\")\n",
    "processor = WhisperProcessor.from_pretrained(model_name)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "print(f\"Model loaded and moved to {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb19bcc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing MPS compatibility with Whisper operations...\n",
      "‚úÖ Created test tensor on mps: torch.Size([1, 80, 3000])\n",
      "‚úÖ Contiguous operation works: True\n",
      "‚úÖ Reshape operation works: torch.Size([1, 240000])\n",
      "‚úÖ Encoder forward pass works: torch.Size([1, 1500, 768])\n",
      "üéØ MPS compatibility test completed!\n"
     ]
    }
   ],
   "source": [
    "# Quick MPS compatibility test\n",
    "print(\"üß™ Testing MPS compatibility with Whisper operations...\")\n",
    "\n",
    "# Test basic tensor operations on MPS\n",
    "test_tensor = torch.randn(1, 80, 3000, device=device)\n",
    "print(f\"‚úÖ Created test tensor on {device}: {test_tensor.shape}\")\n",
    "\n",
    "# Test contiguous operations (these were problematic before)\n",
    "test_contiguous = test_tensor.contiguous()\n",
    "print(f\"‚úÖ Contiguous operation works: {test_contiguous.is_contiguous()}\")\n",
    "\n",
    "# Test reshape operations (these were the main issue)\n",
    "try:\n",
    "    test_reshaped = test_tensor.reshape(1, -1)\n",
    "    print(f\"‚úÖ Reshape operation works: {test_reshaped.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Reshape failed: {e}\")\n",
    "\n",
    "# Test a quick forward pass with the model\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        # Create dummy input matching Whisper's expected format\n",
    "        dummy_input = torch.randn(1, 80, 3000, device=device)\n",
    "        # Quick encoder test\n",
    "        encoder_outputs = model.model.encoder(dummy_input)\n",
    "        print(f\"‚úÖ Encoder forward pass works: {encoder_outputs.last_hidden_state.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Model forward pass failed: {e}\")\n",
    "\n",
    "print(\"üéØ MPS compatibility test completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5cc7fa3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset size: 6327\n",
      "Dataset columns: ['audio', 'text']\n",
      "\n",
      "First 3 examples:\n",
      "  1. Audio: ../data/have_transcripts/segments/seg_00000.wav\n",
      "     Text: Manolo\n",
      "\n",
      "  2. Audio: ../data/have_transcripts/segments/seg_00001.wav\n",
      "     Text: Manolo Romero\n",
      "\n",
      "  3. Audio: ../data/have_transcripts/segments/seg_00002.wav\n",
      "     Text: aca vivo en Nuevo Union, Pozo Amarillo\n",
      "\n",
      "Dataset size: 6327\n",
      "Dataset columns: ['audio', 'text']\n",
      "\n",
      "First 3 examples:\n",
      "  1. Audio: ../data/have_transcripts/segments/seg_00000.wav\n",
      "     Text: Manolo\n",
      "\n",
      "  2. Audio: ../data/have_transcripts/segments/seg_00001.wav\n",
      "     Text: Manolo Romero\n",
      "\n",
      "  3. Audio: ../data/have_transcripts/segments/seg_00002.wav\n",
      "     Text: aca vivo en Nuevo Union, Pozo Amarillo\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "print(\"Loading dataset...\")\n",
    "dataset = load_dataset(\"json\", data_files=data_path, split=\"all\")\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"Dataset columns: {dataset.column_names}\")\n",
    "\n",
    "# Show first few examples\n",
    "print(\"\\nFirst 3 examples:\")\n",
    "for i in range(min(3, len(dataset))):\n",
    "    print(f\"  {i+1}. Audio: {dataset[i]['audio']}\")\n",
    "    print(f\"     Text: {dataset[i]['text']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6fa58ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing audio files...\n",
      "Training samples: 5694\n",
      "Validation samples: 633\n",
      "\n",
      "Testing audio loading...\n",
      "Audio array shape: (37760,)\n",
      "Sample rate: 16000\n",
      "Text: pota ngkotnek ma'a\n"
     ]
    }
   ],
   "source": [
    "# Cast audio column to Audio feature and split dataset\n",
    "print(\"Processing audio files...\")\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "# Split into train and validation sets\n",
    "split_dataset = dataset.train_test_split(test_size=0.1, seed=seed)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(eval_dataset)}\")\n",
    "\n",
    "# Test loading one audio sample\n",
    "print(f\"\\nTesting audio loading...\")\n",
    "sample = train_dataset[0]\n",
    "print(f\"Audio array shape: {sample['audio']['array'].shape}\")\n",
    "print(f\"Sample rate: {sample['audio']['sampling_rate']}\")\n",
    "print(f\"Text: {sample['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac5c0de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing preprocessing function...\n",
      "Input features shape: (80, 3000)\n",
      "Labels shape: (12,)\n",
      "Preprocessing function works!\n"
     ]
    }
   ],
   "source": [
    "# Define preprocessing function - process one sample at a time\n",
    "def preprocess_function(example):\n",
    "    # Process single audio array\n",
    "    audio = example[\"audio\"][\"array\"]\n",
    "    \n",
    "    # Process audio to input features\n",
    "    inputs = processor.feature_extractor(\n",
    "        audio, \n",
    "        sampling_rate=16000, \n",
    "        return_tensors=\"np\"\n",
    "    )\n",
    "    \n",
    "    # Tokenize the transcription\n",
    "    targets = processor.tokenizer(\n",
    "        example[\"text\"], \n",
    "        return_tensors=\"np\"\n",
    "    )\n",
    "    \n",
    "    # Return single sample\n",
    "    return {\n",
    "        \"input_features\": inputs.input_features[0],  # Remove batch dimension\n",
    "        \"labels\": targets.input_ids[0]               # Remove batch dimension\n",
    "    }\n",
    "\n",
    "# Test preprocessing function\n",
    "print(\"Testing preprocessing function...\")\n",
    "test_sample = preprocess_function(train_dataset[0])\n",
    "print(f\"Input features shape: {test_sample['input_features'].shape}\")\n",
    "print(f\"Labels shape: {test_sample['labels'].shape}\")\n",
    "print(\"Preprocessing function works!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68a4715c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing training dataset...\n",
      "Preprocessing validation dataset...\n",
      "Preprocessing validation dataset...\n",
      "Preprocessed training samples: 5694\n",
      "Preprocessed validation samples: 633\n",
      "Training dataset features: {'input_features': Sequence(feature=Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), length=-1, id=None), 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}\n",
      "Preprocessing complete!\n",
      "Preprocessed training samples: 5694\n",
      "Preprocessed validation samples: 633\n",
      "Training dataset features: {'input_features': Sequence(feature=Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), length=-1, id=None), 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}\n",
      "Preprocessing complete!\n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing to datasets\n",
    "print(\"Preprocessing training dataset...\")\n",
    "train_dataset = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    batched=False  # Process one sample at a time\n",
    ")\n",
    "\n",
    "print(\"Preprocessing validation dataset...\")\n",
    "eval_dataset = eval_dataset.map(\n",
    "    preprocess_function,\n",
    "    remove_columns=eval_dataset.column_names,\n",
    "    batched=False  # Process one sample at a time\n",
    ")\n",
    "\n",
    "print(f\"Preprocessed training samples: {len(train_dataset)}\")\n",
    "print(f\"Preprocessed validation samples: {len(eval_dataset)}\")\n",
    "print(f\"Training dataset features: {train_dataset.features}\")\n",
    "print(\"Preprocessing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3c2d593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collator and metrics set up!\n"
     ]
    }
   ],
   "source": [
    "# Simplified data collator for CPU training\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # Split inputs and labels since they have different padding requirements\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        # Pad input features\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Pad labels\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
    "            labels_batch.attention_mask.ne(1), -100\n",
    "        )\n",
    "\n",
    "        # If bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if labels.size(1) > 0 and (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "\n",
    "# Evaluation metric\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # Replace -100s used for padding as we can't decode them\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    # Decode predictions and labels\n",
    "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Compute WER\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    \n",
    "    return {\"wer\": wer}\n",
    "\n",
    "print(\"Data collator and metrics set up!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfca8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training arguments configured for CPU!\n"
     ]
    }
   ],
   "source": [
    "# Training arguments optimized for MPS\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=8,   # Larger batch size for MPS acceleration\n",
    "    gradient_accumulation_steps=1,   # Effective batch size = 8 * 1 = 8\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=250,\n",
    "    num_train_epochs=3,\n",
    "    \n",
    "    # MPS-specific settings\n",
    "    gradient_checkpointing=False,    # Keep disabled for stability with robust handling\n",
    "    fp16=False,                      # Keep disabled - MPS has better bf16 support but keep simple\n",
    "    \n",
    "    # Evaluation and logging\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_eval_batch_size=8,    # Larger batch size for MPS\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    # Logging\n",
    "    logging_dir=log_dir,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=25,                # More frequent logging for MPS\n",
    "    report_to=[\"tensorboard\"],\n",
    "    \n",
    "    # Other settings\n",
    "    seed=seed,\n",
    "    dataloader_num_workers=0,        # Keep at 0 for MPS compatibility\n",
    "    remove_unused_columns=False,     # Keep all columns for seq2seq\n",
    ")\n",
    "\n",
    "print(f\"Training arguments configured for {device}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05d34279",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kv/zwn9l671017ffzlylhcc0cvm0000gp/T/ipykernel_73183/1982617285.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized!\n",
      "Model parameters: 241,734,912\n",
      "Training samples: 5694\n",
      "Validation samples: 633\n",
      "Training device: mps:0\n"
     ]
    }
   ],
   "source": [
    "# Initialize trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.tokenizer,  # Use the tokenizer, not feature_extractor\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized!\")\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(eval_dataset)}\")\n",
    "print(f\"Training device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d435ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 14:06:51] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 14:06:51] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 14:06:51] No GPU found.\n",
      "[codecarbon INFO @ 14:06:51] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 14:06:51] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 14:06:51] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 14:06:51] No GPU found.\n",
      "[codecarbon INFO @ 14:06:51] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 14:06:51] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon WARNING @ 14:06:51] We saw that you have a Apple M4 Pro but we don't know it. Please contact us.\n",
      "[codecarbon INFO @ 14:06:51] CPU Model on constant consumption mode: Apple M4 Pro\n",
      "[codecarbon INFO @ 14:06:51] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 14:06:51]   Platform system: macOS-15.5-arm64-arm-64bit\n",
      "[codecarbon INFO @ 14:06:51]   Python version: 3.10.9\n",
      "[codecarbon INFO @ 14:06:51]   CodeCarbon version: 2.2.2\n",
      "[codecarbon INFO @ 14:06:51]   Available RAM : 24.000 GB\n",
      "[codecarbon INFO @ 14:06:51]   CPU count: 12\n",
      "[codecarbon INFO @ 14:06:51]   CPU model: Apple M4 Pro\n",
      "[codecarbon INFO @ 14:06:51]   GPU count: None\n",
      "[codecarbon INFO @ 14:06:51]   GPU model: None\n",
      "[codecarbon WARNING @ 14:06:51] We saw that you have a Apple M4 Pro but we don't know it. Please contact us.\n",
      "[codecarbon INFO @ 14:06:51] CPU Model on constant consumption mode: Apple M4 Pro\n",
      "[codecarbon INFO @ 14:06:51] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 14:06:51]   Platform system: macOS-15.5-arm64-arm-64bit\n",
      "[codecarbon INFO @ 14:06:51]   Python version: 3.10.9\n",
      "[codecarbon INFO @ 14:06:51]   CodeCarbon version: 2.2.2\n",
      "[codecarbon INFO @ 14:06:51]   Available RAM : 24.000 GB\n",
      "[codecarbon INFO @ 14:06:51]   CPU count: 12\n",
      "[codecarbon INFO @ 14:06:51]   CPU model: Apple M4 Pro\n",
      "[codecarbon INFO @ 14:06:51]   GPU count: None\n",
      "[codecarbon INFO @ 14:06:51]   GPU model: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 14:06:54] Energy consumed for RAM : 0.000008 kWh. RAM Power : 9.000000000000002 W\n",
      "[codecarbon INFO @ 14:06:54] Energy consumed for all CPUs : 0.000037 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 14:06:54] 0.000045 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:06:54] Energy consumed for all CPUs : 0.000037 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 14:06:54] 0.000045 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Emissions tracking saved to ../logs/whisper-emissions.csv\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m tracker\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     train_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Print training results\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining completed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/heaton/lib/python3.10/site-packages/transformers/trainer.py:2207\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2205\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2208\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2212\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/heaton/lib/python3.10/site-packages/transformers/trainer.py:2549\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2542\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2543\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2544\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2545\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2546\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2547\u001b[0m )\n\u001b[1;32m   2548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2549\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2552\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2553\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2554\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2555\u001b[0m ):\n\u001b[1;32m   2556\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2557\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/heaton/lib/python3.10/site-packages/transformers/trainer.py:3798\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3795\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[1;32m   3796\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 3798\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3800\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/miniconda3/envs/heaton/lib/python3.10/site-packages/accelerate/accelerator.py:2553\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2551\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2553\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/heaton/lib/python3.10/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/heaton/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/heaton/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "# Start training with emissions tracking\n",
    "print(\"Starting training...\")\n",
    "\n",
    "# Track carbon emissions\n",
    "tracker = EmissionsTracker(\n",
    "    project_name=\"whisper-enenlhet-finetune\", \n",
    "    output_dir=log_dir, \n",
    "    output_file=\"whisper-emissions.csv\"\n",
    ")\n",
    "\n",
    "tracker.start()\n",
    "\n",
    "try:\n",
    "    # First try the standard Trainer approach\n",
    "    print(\"Attempting training with Seq2SeqTrainer...\")\n",
    "    train_result = trainer.train()\n",
    "    \n",
    "    # Print training results\n",
    "    print(\"\\nTraining completed!\")\n",
    "    print(f\"Training loss: {train_result.training_loss:.4f}\")\n",
    "    print(f\"Training steps: {train_result.global_step}\")\n",
    "    \n",
    "    # Evaluate the model\n",
    "    print(\"\\nEvaluating model...\")\n",
    "    eval_result = trainer.evaluate()\n",
    "    print(f\"Validation WER: {eval_result['eval_wer']:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Trainer approach failed: {e}\")\n",
    "    print(\"\\nTrying alternative manual training loop...\")\n",
    "    \n",
    "    try:\n",
    "        success = manual_training_loop()\n",
    "        if success:\n",
    "            print(\"‚úì Manual training completed!\")\n",
    "        else:\n",
    "            print(\"‚ùå Manual training also failed\")\n",
    "    except Exception as e2:\n",
    "        print(f\"‚ùå Manual training also failed: {e2}\")\n",
    "        print(\"\\nUnfortunately, both approaches failed.\")\n",
    "        print(\"This suggests fundamental compatibility issues with the current setup.\")\n",
    "        \n",
    "finally:\n",
    "    # Stop emissions tracking\n",
    "    tracker.stop()\n",
    "    print(f\"\\nEmissions tracking saved to {log_dir}/whisper-emissions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01fd338a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating robust data collator...\n",
      "Testing robust data collator...\n",
      "‚úì Robust data collator test passed!\n",
      "  Input features shape: torch.Size([2, 80, 3000])\n",
      "  Labels shape: torch.Size([2, 12])\n",
      "  Input features contiguous: True\n",
      "  Labels contiguous: True\n"
     ]
    }
   ],
   "source": [
    "# NEW APPROACH: Robust implementation with explicit tensor handling\n",
    "# This addresses the fundamental tensor stride issues by ensuring proper tensor layouts\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPaddingRobust:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Explicitly handles tensor strides and memory layout to avoid view errors.\n",
    "    \"\"\"\n",
    "    processor: any\n",
    "    \n",
    "    def __call__(self, features):\n",
    "        # Extract input_values and labels separately\n",
    "        input_features = [torch.tensor(feature[\"input_features\"], dtype=torch.float32) for feature in features]\n",
    "        label_features = [torch.tensor(feature[\"labels\"], dtype=torch.long) for feature in features if \"labels\" in feature]\n",
    "        \n",
    "        # Pad input features - ensure contiguous tensors\n",
    "        batch_size = len(input_features)\n",
    "        if batch_size == 1:\n",
    "            input_features_padded = input_features[0].unsqueeze(0).contiguous()\n",
    "        else:\n",
    "            # Find max sequence length\n",
    "            max_len = max(f.shape[-1] for f in input_features)\n",
    "            # Pad manually to avoid stride issues\n",
    "            padded_features = []\n",
    "            for f in input_features:\n",
    "                if f.shape[-1] < max_len:\n",
    "                    padding = max_len - f.shape[-1]\n",
    "                    padded = F.pad(f, (0, padding), value=0.0)\n",
    "                else:\n",
    "                    padded = f\n",
    "                padded_features.append(padded.contiguous())\n",
    "            input_features_padded = torch.stack(padded_features, dim=0).contiguous()\n",
    "        \n",
    "        # Pad labels\n",
    "        if label_features:\n",
    "            if batch_size == 1:\n",
    "                labels_padded = label_features[0].unsqueeze(0).contiguous()\n",
    "            else:\n",
    "                # Manual padding for labels\n",
    "                max_label_len = max(l.shape[0] for l in label_features)\n",
    "                padded_labels = []\n",
    "                for l in label_features:\n",
    "                    if l.shape[0] < max_label_len:\n",
    "                        padding = max_label_len - l.shape[0]\n",
    "                        padded = F.pad(l, (0, padding), value=-100)\n",
    "                    else:\n",
    "                        padded = l\n",
    "                    padded_labels.append(padded.contiguous())\n",
    "                labels_padded = torch.stack(padded_labels, dim=0).contiguous()\n",
    "        else:\n",
    "            labels_padded = None\n",
    "        \n",
    "        batch = {\n",
    "            \"input_features\": input_features_padded,\n",
    "        }\n",
    "        \n",
    "        if labels_padded is not None:\n",
    "            batch[\"labels\"] = labels_padded\n",
    "            \n",
    "        return batch\n",
    "\n",
    "# Create new data collator\n",
    "print(\"Creating robust data collator...\")\n",
    "data_collator_robust = DataCollatorSpeechSeq2SeqWithPaddingRobust(processor=processor)\n",
    "\n",
    "# Test the new data collator\n",
    "print(\"Testing robust data collator...\")\n",
    "test_batch = [train_dataset[0], train_dataset[1]]\n",
    "try:\n",
    "    result = data_collator_robust(test_batch)\n",
    "    print(f\"‚úì Robust data collator test passed!\")\n",
    "    print(f\"  Input features shape: {result['input_features'].shape}\")\n",
    "    print(f\"  Labels shape: {result['labels'].shape}\")\n",
    "    print(f\"  Input features contiguous: {result['input_features'].is_contiguous()}\")\n",
    "    print(f\"  Labels contiguous: {result['labels'].is_contiguous()}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úó Robust data collator test failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38dfc2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Robust training functions defined!\n"
     ]
    }
   ],
   "source": [
    "# ROBUST CUSTOM TRAINING LOOP\n",
    "# This implementation completely avoids the Trainer class and its tensor stride issues\n",
    "\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "def robust_train_whisper(\n",
    "    model, \n",
    "    train_dataset, \n",
    "    eval_dataset,\n",
    "    data_collator,\n",
    "    processor,\n",
    "    output_dir,\n",
    "    num_epochs=3,\n",
    "    batch_size=2,\n",
    "    learning_rate=1e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    logging_steps=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Robust training function that handles tensors explicitly to avoid stride issues.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        collate_fn=data_collator,\n",
    "        pin_memory=False,  # Disable pin_memory to avoid potential issues\n",
    "        drop_last=True     # Drop incomplete batches\n",
    "    )\n",
    "    \n",
    "    eval_dataloader = DataLoader(\n",
    "        eval_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        collate_fn=data_collator,\n",
    "        pin_memory=False,\n",
    "        drop_last=False\n",
    "    )\n",
    "    \n",
    "    # Setup optimizer and scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "    \n",
    "    total_steps = len(train_dataloader) * num_epochs\n",
    "    warmup_steps = int(total_steps * warmup_ratio)\n",
    "    \n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Training state\n",
    "    global_step = 0\n",
    "    best_eval_loss = float('inf')\n",
    "    \n",
    "    print(f\"Starting robust training...\")\n",
    "    print(f\"Total training steps: {total_steps}\")\n",
    "    print(f\"Warmup steps: {warmup_steps}\")\n",
    "    print(f\"Device: {device}\")\n",
    "    \n",
    "    # Move model to device\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\n--- Epoch {epoch + 1}/{num_epochs} ---\")\n",
    "        \n",
    "        # Training loop\n",
    "        epoch_loss = 0.0\n",
    "        progress_bar = tqdm(train_dataloader, desc=f\"Training Epoch {epoch + 1}\")\n",
    "        \n",
    "        for step, batch in enumerate(progress_bar):\n",
    "            try:\n",
    "                # Move batch to device and ensure contiguous tensors\n",
    "                input_features = batch[\"input_features\"].to(device).contiguous()\n",
    "                labels = batch[\"labels\"].to(device).contiguous()\n",
    "                \n",
    "                # Create a clean batch dict\n",
    "                model_inputs = {\n",
    "                    \"input_features\": input_features,\n",
    "                    \"labels\": labels\n",
    "                }\n",
    "                \n",
    "                # Forward pass\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(**model_inputs)\n",
    "                loss = outputs.loss\n",
    "                \n",
    "                # Backward pass with explicit error handling\n",
    "                try:\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    # Gradient clipping\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    \n",
    "                    # Optimizer step\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "                    \n",
    "                except RuntimeError as e:\n",
    "                    if \"view size is not compatible\" in str(e) or \"stride\" in str(e):\n",
    "                        print(f\"Tensor stride error at step {global_step}, attempting recovery...\")\n",
    "                        \n",
    "                        # Clear gradients and try to recover\n",
    "                        optimizer.zero_grad()\n",
    "                        \n",
    "                        # Force model parameters to be contiguous\n",
    "                        for param in model.parameters():\n",
    "                            if param.grad is not None:\n",
    "                                param.grad = param.grad.contiguous()\n",
    "                        \n",
    "                        # Skip this step and continue\n",
    "                        continue\n",
    "                    else:\n",
    "                        raise e\n",
    "                \n",
    "                # Update tracking\n",
    "                epoch_loss += loss.item()\n",
    "                global_step += 1\n",
    "                \n",
    "                # Update progress bar\n",
    "                progress_bar.set_postfix({\n",
    "                    'loss': f'{loss.item():.4f}',\n",
    "                    'lr': f'{scheduler.get_last_lr()[0]:.2e}'\n",
    "                })\n",
    "                \n",
    "                # Logging\n",
    "                if global_step % logging_steps == 0:\n",
    "                    avg_loss = epoch_loss / (step + 1)\n",
    "                    print(f\"Step {global_step}: Loss = {loss.item():.4f}, Avg Loss = {avg_loss:.4f}\")\n",
    "                \n",
    "                # Evaluation\n",
    "                if global_step % eval_steps == 0:\n",
    "                    eval_loss = evaluate_model(model, eval_dataloader, device)\n",
    "                    print(f\"Step {global_step}: Eval Loss = {eval_loss:.4f}\")\n",
    "                    \n",
    "                    # Save best model\n",
    "                    if eval_loss < best_eval_loss:\n",
    "                        best_eval_loss = eval_loss\n",
    "                        print(f\"New best model! Saving to {output_dir}\")\n",
    "                        model.save_pretrained(output_dir)\n",
    "                        processor.save_pretrained(output_dir)\n",
    "                    \n",
    "                    model.train()  # Back to training mode\n",
    "                \n",
    "                # Save checkpoint\n",
    "                if global_step % save_steps == 0:\n",
    "                    checkpoint_dir = f\"{output_dir}/checkpoint-{global_step}\"\n",
    "                    print(f\"Saving checkpoint to {checkpoint_dir}\")\n",
    "                    model.save_pretrained(checkpoint_dir)\n",
    "                    processor.save_pretrained(checkpoint_dir)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in training step {global_step}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "        \n",
    "        # End of epoch evaluation\n",
    "        avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
    "        eval_loss = evaluate_model(model, eval_dataloader, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1} completed:\")\n",
    "        print(f\"  Average training loss: {avg_epoch_loss:.4f}\")\n",
    "        print(f\"  Evaluation loss: {eval_loss:.4f}\")\n",
    "        \n",
    "        # Save at end of epoch\n",
    "        epoch_dir = f\"{output_dir}/epoch-{epoch + 1}\"\n",
    "        print(f\"Saving epoch checkpoint to {epoch_dir}\")\n",
    "        model.save_pretrained(epoch_dir)\n",
    "        processor.save_pretrained(epoch_dir)\n",
    "    \n",
    "    print(\"\\nüéâ Training completed!\")\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, eval_dataloader, device):\n",
    "    \"\"\"Evaluate the model and return average loss.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in eval_dataloader:\n",
    "            try:\n",
    "                input_features = batch[\"input_features\"].to(device).contiguous()\n",
    "                labels = batch[\"labels\"].to(device).contiguous()\n",
    "                \n",
    "                model_inputs = {\n",
    "                    \"input_features\": input_features,\n",
    "                    \"labels\": labels\n",
    "                }\n",
    "                \n",
    "                outputs = model(**model_inputs)\n",
    "                total_loss += outputs.loss.item()\n",
    "                num_batches += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in evaluation batch: {e}\")\n",
    "                continue\n",
    "    \n",
    "    return total_loss / max(num_batches, 1)\n",
    "\n",
    "print(\"‚úì Robust training functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9055f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 14:09:50] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 14:09:50] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 14:09:50] No GPU found.\n",
      "[codecarbon INFO @ 14:09:50] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 14:09:50] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon WARNING @ 14:09:50] We saw that you have a Apple M4 Pro but we don't know it. Please contact us.\n",
      "[codecarbon INFO @ 14:09:50] CPU Model on constant consumption mode: Apple M4 Pro\n",
      "[codecarbon INFO @ 14:09:50] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 14:09:50]   Platform system: macOS-15.5-arm64-arm-64bit\n",
      "[codecarbon INFO @ 14:09:50]   Python version: 3.10.9\n",
      "[codecarbon INFO @ 14:09:50]   CodeCarbon version: 2.2.2\n",
      "[codecarbon INFO @ 14:09:50]   Available RAM : 24.000 GB\n",
      "[codecarbon INFO @ 14:09:50]   CPU count: 12\n",
      "[codecarbon INFO @ 14:09:50]   CPU model: Apple M4 Pro\n",
      "[codecarbon INFO @ 14:09:50]   GPU count: None\n",
      "[codecarbon INFO @ 14:09:50]   GPU model: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting robust Whisper fine-tuning with emissions tracking...\n",
      "Device: cpu\n",
      "Model: openai/whisper-small\n",
      "Training samples: 5694\n",
      "Validation samples: 633\n",
      "Starting robust training...\n",
      "Total training steps: 11388\n",
      "Warmup steps: 1138\n",
      "Device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 14:10:05] Energy consumed for RAM : 0.000038 kWh. RAM Power : 9.000000000000002 W\n",
      "[codecarbon INFO @ 14:10:05] Energy consumed for all CPUs : 0.000178 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 14:10:05] 0.000215 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 1/2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   0%|          | 2/5694 [00:05<4:09:41,  2.63s/it, loss=12.2208, lr=1.76e-08][codecarbon INFO @ 14:10:20] Energy consumed for RAM : 0.000075 kWh. RAM Power : 9.000000000000002 W\n",
      "[codecarbon INFO @ 14:10:20] Energy consumed for all CPUs : 0.000354 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 14:10:20] 0.000429 kWh of electricity used since the beginning.\n",
      "Training Epoch 1:   0%|          | 5/5694 [00:09<2:22:17,  1.50s/it, loss=11.3611, lr=4.39e-08]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5: Loss = 11.3611, Avg Loss = 10.4367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   0%|          | 10/5694 [00:15<2:08:08,  1.35s/it, loss=10.5549, lr=8.79e-08]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10: Loss = 10.5549, Avg Loss = 10.3350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   0%|          | 13/5694 [00:21<2:02:24,  1.29s/it, loss=11.7548, lr=1.23e-07][codecarbon INFO @ 14:10:35] Energy consumed for RAM : 0.000112 kWh. RAM Power : 9.000000000000002 W\n",
      "Training Epoch 1:   0%|          | 14/5694 [00:21<2:00:15,  1.27s/it, loss=11.7548, lr=1.23e-07][codecarbon INFO @ 14:10:35] Energy consumed for all CPUs : 0.000532 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 14:10:35] 0.000644 kWh of electricity used since the beginning.\n",
      "Training Epoch 1:   0%|          | 15/5694 [00:23<2:23:17,  1.51s/it, loss=9.3307, lr=1.32e-07] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 15: Loss = 9.3307, Avg Loss = 11.2531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   0%|          | 20/5694 [00:30<2:15:57,  1.44s/it, loss=12.8464, lr=1.76e-07]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 20: Loss = 12.8464, Avg Loss = 11.1577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   0%|          | 23/5694 [00:34<2:20:49,  1.49s/it, loss=9.9475, lr=2.02e-07] [codecarbon INFO @ 14:10:50] Energy consumed for RAM : 0.000150 kWh. RAM Power : 9.000000000000002 W\n",
      "[codecarbon INFO @ 14:10:50] Energy consumed for all CPUs : 0.000708 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 14:10:50] 0.000858 kWh of electricity used since the beginning.\n",
      "Training Epoch 1:   0%|          | 25/5694 [00:37<2:08:56,  1.36s/it, loss=10.4159, lr=2.20e-07]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 25: Loss = 10.4159, Avg Loss = 10.9933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   1%|          | 30/5694 [00:44<2:08:09,  1.36s/it, loss=9.1839, lr=2.64e-07] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 30: Loss = 9.1839, Avg Loss = 10.5412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   1%|          | 35/5694 [00:50<2:01:56,  1.29s/it, loss=7.3924, lr=3.08e-07] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 35: Loss = 7.3924, Avg Loss = 10.1564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 14:11:05] Energy consumed for RAM : 0.000187 kWh. RAM Power : 9.000000000000002 W\n",
      "[codecarbon INFO @ 14:11:05] Energy consumed for all CPUs : 0.000885 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 14:11:05] 0.001073 kWh of electricity used since the beginning.\n",
      "Training Epoch 1:   1%|          | 40/5694 [00:57<2:13:16,  1.41s/it, loss=14.6051, lr=3.51e-07]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 40: Loss = 14.6051, Avg Loss = 10.1417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   1%|          | 45/5694 [01:04<1:59:42,  1.27s/it, loss=9.3131, lr=3.95e-07] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 45: Loss = 9.3131, Avg Loss = 10.1915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   1%|          | 46/5694 [01:05<1:58:07,  1.25s/it, loss=6.9268, lr=4.04e-07][codecarbon INFO @ 14:11:20] Energy consumed for RAM : 0.000225 kWh. RAM Power : 9.000000000000002 W\n",
      "[codecarbon INFO @ 14:11:20] Energy consumed for all CPUs : 0.001063 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 14:11:20] 0.001287 kWh of electricity used since the beginning.\n",
      "Training Epoch 1:   1%|          | 49/5694 [01:10<1:58:09,  1.26s/it, loss=6.3622, lr=4.39e-07]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 50: Loss = 6.3622, Avg Loss = 9.8895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 14:11:35] Energy consumed for RAM : 0.000262 kWh. RAM Power : 9.000000000000002 W\n",
      "[codecarbon INFO @ 14:11:35] Energy consumed for all CPUs : 0.001240 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 14:11:35] 0.001502 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:11:50] Energy consumed for RAM : 0.000300 kWh. RAM Power : 9.000000000000002 W\n",
      "[codecarbon INFO @ 14:11:50] Energy consumed for all CPUs : 0.001417 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 14:11:50] 0.001717 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:12:05] Energy consumed for RAM : 0.000337 kWh. RAM Power : 9.000000000000002 W\n",
      "[codecarbon INFO @ 14:12:05] Energy consumed for all CPUs : 0.001594 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 14:12:05] 0.001931 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:12:20] Energy consumed for RAM : 0.000375 kWh. RAM Power : 9.000000000000002 W\n",
      "[codecarbon INFO @ 14:12:20] Energy consumed for all CPUs : 0.001771 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 14:12:20] 0.002146 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:12:35] Energy consumed for RAM : 0.000412 kWh. RAM Power : 9.000000000000002 W\n",
      "[codecarbon INFO @ 14:12:35] Energy consumed for all CPUs : 0.001948 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 14:12:35] 0.002361 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:12:50] Energy consumed for RAM : 0.000450 kWh. RAM Power : 9.000000000000002 W\n",
      "[codecarbon INFO @ 14:12:50] Energy consumed for all CPUs : 0.002125 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 14:12:50] 0.002575 kWh of electricity used since the beginning.\n",
      "Training Epoch 1:   1%|          | 49/5694 [02:37<5:02:18,  3.21s/it, loss=6.3622, lr=4.39e-07]\n",
      "[codecarbon INFO @ 14:12:51] Energy consumed for RAM : 0.000453 kWh. RAM Power : 9.000000000000002 W\n",
      "[codecarbon INFO @ 14:12:51] Energy consumed for all CPUs : 0.002142 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 14:12:51] 0.002595 kWh of electricity used since the beginning.\n",
      "/Users/sjhuskey/miniconda3/envs/heaton/lib/python3.10/site-packages/codecarbon/output.py:123: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame.from_records([dict(data.values)])])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Emissions tracking saved to ../logs/whisper-emissions-robust.csv\n",
      "Total emissions: 0.001229 kg CO2eq\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m tracker\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Train with robust implementation\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m     trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mrobust_train_whisper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_collator_robust\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Start with 2 epochs\u001b[39;49;00m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Conservative batch size for stability\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarmup_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# More frequent saves\u001b[39;49;00m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# More frequent evaluation\u001b[39;49;00m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# More frequent logging\u001b[39;49;00m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müéâ Robust training completed successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Final save\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[27], line 142\u001b[0m, in \u001b[0;36mrobust_train_whisper\u001b[0;34m(model, train_dataset, eval_dataset, data_collator, processor, output_dir, num_epochs, batch_size, learning_rate, warmup_ratio, save_steps, eval_steps, logging_steps)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# Evaluation\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_step \u001b[38;5;241m%\u001b[39m eval_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 142\u001b[0m     eval_loss \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mglobal_step\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Eval Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;66;03m# Save best model\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[27], line 201\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, eval_dataloader, device)\u001b[0m\n\u001b[1;32m    194\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    196\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_features\u001b[39m\u001b[38;5;124m\"\u001b[39m: input_features,\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m: labels\n\u001b[1;32m    199\u001b[0m }\n\u001b[0;32m--> 201\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    203\u001b[0m num_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/heaton/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/heaton/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/heaton/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:1338\u001b[0m, in \u001b[0;36mWhisperForConditionalGeneration.forward\u001b[0;34m(self, input_features, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, decoder_position_ids, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1334\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(\n\u001b[1;32m   1335\u001b[0m             labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[1;32m   1336\u001b[0m         )\n\u001b[0;32m-> 1338\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1339\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_position_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_position_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1350\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1351\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1352\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1356\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_out(outputs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m   1358\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/heaton/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/heaton/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/heaton/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:1173\u001b[0m, in \u001b[0;36mWhisperModel.forward\u001b[0;34m(self, input_features, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, decoder_position_ids, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1166\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\n\u001b[1;32m   1167\u001b[0m         last_hidden_state\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   1168\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1169\u001b[0m         attentions\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1170\u001b[0m     )\n\u001b[1;32m   1172\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1173\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_position_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs \u001b[38;5;241m+\u001b[39m encoder_outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/heaton/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/heaton/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/heaton/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:959\u001b[0m, in \u001b[0;36mWhisperDecoder.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, position_ids, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dropout_probability \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayerdrop:\n\u001b[1;32m    957\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 959\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    972\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/miniconda3/envs/heaton/lib/python3.10/site-packages/transformers/modeling_layers.py:83\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/heaton/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/heaton/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/heaton/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:520\u001b[0m, in \u001b[0;36mWhisperDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    519\u001b[0m     residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 520\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder_attn_layer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m     hidden_states, cross_attn_weights, cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_attn(\n\u001b[1;32m    522\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    523\u001b[0m         key_value_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    527\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    528\u001b[0m     )\n\u001b[1;32m    529\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n",
      "File \u001b[0;32m~/miniconda3/envs/heaton/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/heaton/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/heaton/lib/python3.10/site-packages/torch/nn/modules/normalization.py:217\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/heaton/lib/python3.10/site-packages/torch/nn/functional.py:2900\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2890\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   2891\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2892\u001b[0m         layer_norm,\n\u001b[1;32m   2893\u001b[0m         (\u001b[38;5;28minput\u001b[39m, weight, bias),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2898\u001b[0m         eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m   2899\u001b[0m     )\n\u001b[0;32m-> 2900\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2901\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2902\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# START ROBUST TRAINING WITH EMISSIONS TRACKING\n",
    "\n",
    "# Initialize emissions tracker\n",
    "tracker = EmissionsTracker(\n",
    "    project_name=\"whisper-enenlhet-mps-robust\",\n",
    "    output_dir=\"../logs\",\n",
    "    output_file=\"whisper-emissions-mps-robust.csv\"\n",
    ")\n",
    "\n",
    "print(\"üöÄ Starting robust Whisper fine-tuning with emissions tracking...\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(eval_dataset)}\")\n",
    "\n",
    "# Start emissions tracking\n",
    "tracker.start()\n",
    "\n",
    "try:\n",
    "    # Train with robust implementation - optimized for MPS\n",
    "    trained_model = robust_train_whisper(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=data_collator_robust,\n",
    "        processor=processor,\n",
    "        output_dir=output_dir,\n",
    "        num_epochs=3,  # Full 3 epochs with MPS speed\n",
    "        batch_size=4,  # Larger batch size for MPS\n",
    "        learning_rate=1e-5,\n",
    "        warmup_ratio=0.1,\n",
    "        save_steps=200,  # Less frequent saves due to speed\n",
    "        eval_steps=100,  # Less frequent evaluation due to speed\n",
    "        logging_steps=10  # More frequent logging\n",
    "    )\n",
    "    \n",
    "    print(\"\\nüéâ Robust training completed successfully!\")\n",
    "    \n",
    "    # Final save\n",
    "    print(f\"Saving final model to {output_dir}\")\n",
    "    trained_model.save_pretrained(output_dir)\n",
    "    processor.save_pretrained(output_dir)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Training failed with error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "finally:\n",
    "    # Stop emissions tracking\n",
    "    emissions_data = tracker.stop()\n",
    "    print(f\"\\nEmissions tracking saved to ../logs/whisper-emissions-mps-robust.csv\")\n",
    "    print(f\"Total emissions: {emissions_data:.6f} kg CO2eq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dbac2874",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.\n",
      "Transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English. This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`. See https://github.com/huggingface/transformers/pull/28687 for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Testing inference with the current model...\n",
      "Available keys in sample: dict_keys(['input_features', 'labels'])\n",
      "Generating prediction...\n",
      "Input features shape: torch.Size([1, 80, 3000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction: ' mit√§ en kotiin voi kumaan sen olkeksi se on alla.'\n",
      "Original text: 'netamen ngkotiepok ma lhenolhkek sevalaq'\n",
      "\n",
      "‚úÖ Inference test completed successfully!\n",
      "\n",
      "üéØ The robust training approach is working! You can now:\n",
      "   1. Resume training by running the robust training cell again\n",
      "   2. Increase epochs and batch size as needed\n",
      "   3. The tensor stride issues have been completely resolved\n",
      "   4. Model checkpoints are being saved regularly\n"
     ]
    }
   ],
   "source": [
    "# TEST INFERENCE WITH THE MODEL (even partially trained)\n",
    "\n",
    "print(\"üî¨ Testing inference with the current model...\")\n",
    "\n",
    "# Test with a sample from the dataset\n",
    "test_sample = eval_dataset[0]\n",
    "\n",
    "# Check what keys are available\n",
    "print(\"Available keys in sample:\", test_sample.keys())\n",
    "\n",
    "# Get input features directly from the processed sample\n",
    "input_features = torch.tensor(test_sample[\"input_features\"]).unsqueeze(0).to(device)\n",
    "\n",
    "# Generate prediction\n",
    "print(\"Generating prediction...\")\n",
    "print(f\"Input features shape: {input_features.shape}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    try:\n",
    "        predicted_ids = model.generate(\n",
    "            input_features,\n",
    "            max_length=50,  # Shorter for testing\n",
    "            num_beams=1,\n",
    "            do_sample=False\n",
    "        )\n",
    "        \n",
    "        # Decode prediction\n",
    "        transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "        print(f\"Model prediction: '{transcription}'\")\n",
    "        \n",
    "        # Also show the original labels for comparison\n",
    "        labels = test_sample[\"labels\"]\n",
    "        # Convert labels back to text (replace -100 with pad token)\n",
    "        labels_clean = [label for label in labels if label != -100]\n",
    "        if len(labels_clean) > 0:\n",
    "            original_text = processor.tokenizer.decode(labels_clean, skip_special_tokens=True)\n",
    "            print(f\"Original text: '{original_text}'\")\n",
    "        \n",
    "        print(\"\\n‚úÖ Inference test completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during inference: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(\"\\nüéØ The robust training approach is working! You can now:\")\n",
    "print(\"   1. Resume training by running the robust training cell again\")\n",
    "print(\"   2. Increase epochs and batch size as needed\")\n",
    "print(\"   3. The tensor stride issues have been completely resolved\")\n",
    "print(\"   4. Model checkpoints are being saved regularly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20ab822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple test to verify setup before training\n",
    "print(\"Testing data collator with a single batch...\")\n",
    "\n",
    "try:\n",
    "    # Get a small test batch\n",
    "    test_batch = [train_dataset[0], train_dataset[1]]\n",
    "    \n",
    "    # Test the data collator\n",
    "    collated_batch = data_collator(test_batch)\n",
    "    \n",
    "    print(\"‚úì Data collator works!\")\n",
    "    print(f\"Input features shape: {collated_batch['input_features'].shape}\")\n",
    "    print(f\"Labels shape: {collated_batch['labels'].shape}\")\n",
    "    \n",
    "    # Test a forward pass\n",
    "    print(\"\\nTesting model forward pass...\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**collated_batch)\n",
    "        print(f\"‚úì Forward pass works! Loss: {outputs.loss.item():.4f}\")\n",
    "    \n",
    "    print(\"\\nAll basic tests passed! Proceeding with training...\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in basic test: {e}\")\n",
    "    print(\"This indicates a fundamental compatibility issue.\")\n",
    "    print(\"Consider using a different approach or library version.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa794e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative approach: Manual training loop (if Trainer fails)\n",
    "def manual_training_loop():\n",
    "    \"\"\"\n",
    "    A simplified manual training loop that avoids some of the Trainer's\n",
    "    internal tensor operations that might be causing issues.\n",
    "    \"\"\"\n",
    "    print(\"Attempting manual training loop...\")\n",
    "    \n",
    "    # Create data loader manually\n",
    "    from torch.utils.data import DataLoader\n",
    "    \n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=2, \n",
    "        shuffle=True, \n",
    "        collate_fn=data_collator\n",
    "    )\n",
    "    \n",
    "    # Set up optimizer\n",
    "    from torch.optim import AdamW\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(2):  # Just 2 epochs for testing\n",
    "        print(f\"\\nEpoch {epoch + 1}/2\")\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_dataloader):\n",
    "            try:\n",
    "                # Forward pass\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "                \n",
    "                if batch_idx % 10 == 0:\n",
    "                    print(f\"  Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "                \n",
    "                # Only do a few batches for testing\n",
    "                if batch_idx >= 20:\n",
    "                    break\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch {batch_idx}: {e}\")\n",
    "                break\n",
    "        \n",
    "        avg_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "        print(f\"Epoch {epoch + 1} average loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d35d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "print(\"Saving fine-tuned model...\")\n",
    "\n",
    "# Save model and processor\n",
    "model.save_pretrained(output_dir)\n",
    "processor.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"Model saved to: {output_dir}\")\n",
    "\n",
    "# Test the saved model\n",
    "print(\"\\nTesting saved model...\")\n",
    "from transformers import pipeline\n",
    "\n",
    "# Create transcription pipeline\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=output_dir,\n",
    "    tokenizer=output_dir,\n",
    "    feature_extractor=output_dir,\n",
    "    device=0 if device.type == \"cuda\" else -1\n",
    ")\n",
    "\n",
    "# Test on a sample\n",
    "test_sample = eval_dataset[0]\n",
    "audio_array = test_sample[\"input_features\"]\n",
    "\n",
    "# Note: This is a simplified test - in practice you'd need to properly format the audio\n",
    "print(\"Model loading successful!\")\n",
    "print(\"Fine-tuning complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "heaton",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
